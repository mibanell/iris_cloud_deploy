{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_train_deploy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqBAr/ko74KPd+7VXLy1A+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxXYgcdhXXA2"
      },
      "source": [
        "# Training and deploying model on Google Cloud Platform\n",
        "\n",
        "## Code based on [this](https://cloud.google.com/vertex-ai/docs/tutorials/train-tensorflow-bigquery) GCP tutorial\n",
        "\n",
        "## Author\n",
        "\n",
        "<a href=\"https://github.com/mibanell\">\n",
        "<img src=\"../img/avatar_square.png\" width=\"50\"/>\n",
        "</a>\n",
        "\n",
        "-------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjdtaSSxbzjs"
      },
      "source": [
        "This notebook contains the necessary code for training and deploying a custom Tensorflow model on Google Cloud Platform using the iris dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK6QxSc070tQ",
        "outputId": "67f3dec8-f629-47f1-aede-0017f2539107"
      },
      "source": [
        "!pip install --upgrade google-cloud-aiplatform\n",
        "!pip install -U google-cloud-storage\n",
        "!pip install -U \"google-cloud-bigquery[all]\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.7/dist-packages (1.4.3)\n",
            "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (2.18.0)\n",
            "Requirement already satisfied: proto-plus>=1.10.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (1.19.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (21.0)\n",
            "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (1.42.2)\n",
            "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (1.31.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.53.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (3.17.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.15.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.35.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.40.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (4.2.2)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.7.2)\n",
            "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.3.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.7/dist-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.1.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-cloud-aiplatform) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.24.3)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (1.42.2)\n",
            "Requirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (1.31.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (3.17.3)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (1.35.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (1.15.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (1.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (2.23.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (21.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (1.53.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (57.4.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (2018.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.2.8)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.7/dist-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage) (1.1.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.10)\n",
            "Requirement already satisfied: google-cloud-bigquery[all] in /usr/local/lib/python3.7/dist-packages (2.18.0)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery[all]) (1.7.2)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery[all]) (21.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery[all]) (2.23.0)\n",
            "Requirement already satisfied: proto-plus>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery[all]) (1.19.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery[all]) (3.17.3)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.23.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery[all]) (1.31.2)\n",
            "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery[all]) (1.3.3)\n",
            "Requirement already satisfied: opentelemetry-api>=0.11b0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery[all]) (1.10a0)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage<3.0.0dev,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery[all]) (2.8.0)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery[all]) (1.1.5)\n",
            "Requirement already satisfied: opentelemetry-sdk>=0.11b0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery[all]) (1.10a0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation>=0.11b0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery[all]) (0.24b0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.32.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery[all]) (1.40.0)\n",
            "Requirement already satisfied: pyarrow<5.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery[all]) (3.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0dev,>=4.7.4 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery[all]) (4.62.2)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery[all]) (1.35.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery[all]) (2018.9)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery[all]) (1.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery[all]) (1.53.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery[all]) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery[all]) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery[all]) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery[all]) (0.2.8)\n",
            "Requirement already satisfied: libcst>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery-storage<3.0.0dev,>=2.0.0->google-cloud-bigquery[all]) (0.3.20)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.7/dist-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery[all]) (1.1.5)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from libcst>=0.2.5->google-cloud-bigquery-storage<3.0.0dev,>=2.0.0->google-cloud-bigquery[all]) (0.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.2 in /usr/local/lib/python3.7/dist-packages (from libcst>=0.2.5->google-cloud-bigquery-storage<3.0.0dev,>=2.0.0->google-cloud-bigquery[all]) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.2 in /usr/local/lib/python3.7/dist-packages (from libcst>=0.2.5->google-cloud-bigquery-storage<3.0.0dev,>=2.0.0->google-cloud-bigquery[all]) (3.7.4.3)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opentelemetry-instrumentation>=0.11b0->google-cloud-bigquery[all]) (1.12.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-cloud-bigquery[all]) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->google-cloud-bigquery[all]) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->google-cloud-bigquery[all]) (2.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery[all]) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery[all]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery[all]) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery[all]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery[all]) (1.24.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-bigquery-storage<3.0.0dev,>=2.0.0->google-cloud-bigquery[all]) (0.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc40rLBo9AQu"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform import gapic as aip\n",
        "\n",
        "from datetime import datetime"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KXf_JPR8G3_"
      },
      "source": [
        "PROJECT_ID = \"mibanell\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko6Xyx8M8-DS"
      },
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on Google Cloud Notebooks, then don't execute this code\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNfBeSo09L1w",
        "outputId": "78e28b41-916d-4825-b7dc-f2f542ed85ee"
      },
      "source": [
        "BUCKET_NAME = \"gs://\" + PROJECT_ID + \"-iris-code\"\n",
        "REGION = \"us-central1\"\n",
        "\n",
        "print(BUCKET_NAME)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://mibanell-iris-code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3uptR_Z9-As",
        "outputId": "9a464657-6080-4c05-a534-648f754989fe"
      },
      "source": [
        "!gsutil mb -p $PROJECT_ID -l $REGION $BUCKET_NAME"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://mibanell-iris-code/...\n",
            "ServiceException: 409 A Cloud Storage bucket named 'mibanell-iris-code' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_PMwvdqAxRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd7bff6-a434-4c11-9d44-13ebe7ca5720"
      },
      "source": [
        "!gsutil ls -al $BUCKET_NAME"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      3509  2021-09-19T10:04:43Z  gs://mibanell-iris-code/aiplatform-2021-09-19-10:04:42.929-aiplatform_custom_trainer_script-0.1.tar.gz#1632045883044336  metageneration=1\n",
            "      3496  2021-09-19T10:17:25Z  gs://mibanell-iris-code/aiplatform-2021-09-19-10:17:24.972-aiplatform_custom_trainer_script-0.1.tar.gz#1632046645083659  metageneration=1\n",
            "                                 gs://mibanell-iris-code/aiplatform-custom-training-2021-09-19-10:17:25.112/\n",
            "TOTAL: 2 objects, 7005 bytes (6.84 KiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYIMvzB1BwL4"
      },
      "source": [
        "# Init aiplatform\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsaZU-ioEvCI"
      },
      "source": [
        "# Set training and deployment Docker images\n",
        "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-6:latest\"\n",
        "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWauaIktF5Qk",
        "outputId": "ebca5325-3cb4-4722-926d-920b3f6c9274"
      },
      "source": [
        "# Set machines\n",
        "MACHINE_TYPE = \"n1-standard\"\n",
        "\n",
        "VCPU = \"4\"\n",
        "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Train machine type\", TRAIN_COMPUTE)\n",
        "\n",
        "MACHINE_TYPE = \"n1-standard\"\n",
        "\n",
        "VCPU = \"4\"\n",
        "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train machine type n1-standard-4\n",
            "Deploy machine type n1-standard-4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5_1x0quGeHk"
      },
      "source": [
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "\n",
        "JOB_NAME = \"custom_job_\" + TIMESTAMP\n",
        "\n",
        "TRAIN_STRATEGY = \"single\"\n",
        "\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "CMDARGS = [\n",
        "    \"--epochs=\" + str(EPOCHS),\n",
        "    \"--batch_size=\" + str(BATCH_SIZE),\n",
        "    \"--distribute=\" + TRAIN_STRATEGY\n",
        "]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTiJyNNScaou",
        "outputId": "9f98abfa-3a59-47e3-b734-3702907511a2"
      },
      "source": [
        "# Create dataset from BigQuery data\n",
        "\n",
        "BQ_SOURCE = \"bq://mibanell.iris.iris\"\n",
        "\n",
        "dataset = aiplatform.TabularDataset.create(\n",
        "    display_name=\"iris-data\", bq_source=BQ_SOURCE\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:google.cloud.aiplatform.datasets.dataset:Creating TabularDataset\n",
            "INFO:google.cloud.aiplatform.datasets.dataset:Create TabularDataset backing LRO: projects/987533379656/locations/us-central1/datasets/414498291485507584/operations/8453791687859240960\n",
            "INFO:google.cloud.aiplatform.datasets.dataset:TabularDataset created. Resource name: projects/987533379656/locations/us-central1/datasets/414498291485507584\n",
            "INFO:google.cloud.aiplatform.datasets.dataset:To use this TabularDataset in another session:\n",
            "INFO:google.cloud.aiplatform.datasets.dataset:ds = aiplatform.TabularDataset('projects/987533379656/locations/us-central1/datasets/414498291485507584')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfZJ8OM0bcY5"
      },
      "source": [
        "## Create training package task.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZgr_GZjKT12",
        "outputId": "6cdc3ba2-bc0a-4e63-b95e-c79ed58ea1d2"
      },
      "source": [
        "%%writefile task.py\n",
        "\n",
        "import argparse\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "\n",
        "# Read environmental variables\n",
        "training_data_uri = os.environ[\"AIP_TRAINING_DATA_URI\"]\n",
        "validation_data_uri = os.environ[\"AIP_VALIDATION_DATA_URI\"]\n",
        "test_data_uri = os.environ[\"AIP_TEST_DATA_URI\"]\n",
        "\n",
        "# Read args\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--epochs', dest='epochs',\n",
        "                    default=10, type=int,\n",
        "                    help='Number of epochs.')\n",
        "parser.add_argument('--batch_size', dest='batch_size',\n",
        "                    default=10, type=int,\n",
        "                    help='Batch size.')\n",
        "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
        "                    help='Distributed training strategy.')\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Single Machine, single compute device\n",
        "strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
        "\n",
        "# Set up training variables\n",
        "LABEL_COLUMN = \"Species\"\n",
        "UNUSED_COLUMNS = ['Id']\n",
        "NA_VALUES = [\"NA\", \".\"]\n",
        "\n",
        "# Possible categorical values\n",
        "SPECIES = ['Iris-versicolor', 'Iris-virginica', 'Iris-setosa']\n",
        "\n",
        "# Set up BigQuery clients\n",
        "bqclient = bigquery.Client()\n",
        "\n",
        "# Download a table\n",
        "def download_table(bq_table_uri: str):\n",
        "    # Remove bq:// prefix if present\n",
        "    prefix = \"bq://\"\n",
        "    if bq_table_uri.startswith(prefix):\n",
        "        bq_table_uri = bq_table_uri[len(prefix):]\n",
        "\n",
        "    table = bigquery.TableReference.from_string(bq_table_uri)\n",
        "    rows = bqclient.list_rows(\n",
        "        table,\n",
        "    )\n",
        "    return rows.to_dataframe(create_bqstorage_client=False)\n",
        "\n",
        "\n",
        "df_train = download_table(training_data_uri)\n",
        "df_validation = download_table(validation_data_uri)\n",
        "df_test = download_table(test_data_uri)\n",
        "\n",
        "# Remove NA values\n",
        "def clean_dataframe(df):\n",
        "    return df.replace(to_replace=NA_VALUES, value=np.NaN).dropna()\n",
        "\n",
        "\n",
        "df_train = clean_dataframe(df_train)\n",
        "df_validation = clean_dataframe(df_validation)\n",
        "\n",
        "_CATEGORICAL_TYPES = {\n",
        "    \"Species\": pd.api.types.CategoricalDtype(categories=SPECIES)\n",
        "}\n",
        "\n",
        "\n",
        "def preprocess(df):\n",
        "    \"\"\"Converts categorical features to numeric. Removes unused columns.\n",
        "\n",
        "    Args:\n",
        "      df: Pandas df with raw data\n",
        "\n",
        "    Returns:\n",
        "      df with preprocessed data\n",
        "    \"\"\"\n",
        "    df = df.drop(columns=UNUSED_COLUMNS)\n",
        "\n",
        "    # Drop rows with NaN's\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Convert integer valued (numeric) columns to floating point\n",
        "    numeric_columns = df.select_dtypes([\"int32\", \"float32\", \"float64\"]).columns\n",
        "    df[numeric_columns] = df[numeric_columns].astype(\"float32\")\n",
        "\n",
        "    # Convert categorical columns to numeric\n",
        "    cat_columns = df.select_dtypes([\"object\"]).columns\n",
        "\n",
        "    df[cat_columns] = df[cat_columns].apply(\n",
        "        lambda x: x.astype(_CATEGORICAL_TYPES[x.name])\n",
        "    )\n",
        "    df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n",
        "    return df\n",
        "\n",
        "\n",
        "def convert_dataframe_to_dataset(\n",
        "    df_train,\n",
        "    df_validation\n",
        "):\n",
        "    df_train = preprocess(df_train)\n",
        "    df_validation = preprocess(df_validation)\n",
        "\n",
        "    df_train_x, df_train_y = df_train, df_train.pop(LABEL_COLUMN)\n",
        "    df_validation_x, df_validation_y = df_validation, df_validation.pop(LABEL_COLUMN)\n",
        "\n",
        "    # Join train_x and eval_x to normalize on overall means and standard\n",
        "    # deviations. Then separate them again.\n",
        "    all_x = pd.concat([df_train_x, df_validation_x], keys=[\"train\", \"eval\"])\n",
        "    # all_x = standardize(all_x, mean_and_std)\n",
        "    df_train_x, df_validation_x = all_x.xs(\"train\"), all_x.xs(\"eval\")\n",
        "\n",
        "    y_train = np.asarray(df_train_y).astype(\"float32\")\n",
        "    y_validation = np.asarray(df_validation_y).astype(\"float32\")\n",
        "\n",
        "    # Convert to numpy representation\n",
        "    x_train = np.asarray(df_train_x)\n",
        "    x_test = np.asarray(df_validation_x)\n",
        "\n",
        "    # Convert to one-hot representation\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(SPECIES))\n",
        "    y_validation = tf.keras.utils.to_categorical(y_validation, num_classes=len(SPECIES))\n",
        "\n",
        "    dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    dataset_validation = tf.data.Dataset.from_tensor_slices((x_test, y_validation))\n",
        "    return (dataset_train, dataset_validation)\n",
        "\n",
        "# Create datasets\n",
        "dataset_train, dataset_validation = convert_dataframe_to_dataset(df_train, df_validation)\n",
        "\n",
        "# Shuffle train set\n",
        "dataset_train = dataset_train.shuffle(len(df_train))\n",
        "\n",
        "def create_model(num_features):\n",
        "    # Create model\n",
        "    Dense = tf.keras.layers.Dense\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.Normalization(axis=None),\n",
        "            Dense(\n",
        "                100,\n",
        "                activation=tf.nn.relu,\n",
        "                kernel_initializer=\"uniform\",\n",
        "                input_dim=num_features,\n",
        "            ),\n",
        "            Dense(75, activation=tf.nn.relu),\n",
        "            Dense(50, activation=tf.nn.relu),\n",
        "            Dense(25, activation=tf.nn.relu),\n",
        "            Dense(3, activation=tf.nn.softmax),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Compile Keras model\n",
        "    optimizer = tf.keras.optimizers.RMSprop(lr=0.001)\n",
        "    model.compile(\n",
        "        loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "with strategy.scope():\n",
        "    model = create_model(num_features=dataset_train._flat_shapes[0].dims[0].value)\n",
        "\n",
        "# Set up datasets\n",
        "NUM_WORKERS = strategy.num_replicas_in_sync\n",
        "# Here the batch size scales up by number of workers since\n",
        "# `tf.data.Dataset.batch` expects the global batch size.\n",
        "GLOBAL_BATCH_SIZE = args.batch_size * NUM_WORKERS\n",
        "dataset_train = dataset_train.batch(GLOBAL_BATCH_SIZE)\n",
        "dataset_validation = dataset_validation.batch(GLOBAL_BATCH_SIZE)\n",
        "\n",
        "# Train the model\n",
        "model.fit(dataset_train, epochs=args.epochs, validation_data=dataset_validation)\n",
        "\n",
        "tf.saved_model.save(model, os.environ[\"AIP_MODEL_DIR\"])\n",
        "\n",
        "df_test.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing task.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPolKvtGbsEk"
      },
      "source": [
        "## Create training pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2px_Er_b6m3",
        "outputId": "543829f4-921a-497b-c01c-f37a8cc71d82"
      },
      "source": [
        "job = aiplatform.CustomTrainingJob(\n",
        "    display_name=JOB_NAME,\n",
        "    script_path=\"task.py\",\n",
        "    container_uri=TRAIN_IMAGE,\n",
        "    requirements=[\"google-cloud-bigquery>=2.20.0\"],\n",
        "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
        ")\n",
        "\n",
        "MODEL_DISPLAY_NAME = \"iris-\" + TIMESTAMP\n",
        "\n",
        "# Start the training\n",
        "model = job.run(\n",
        "        dataset=dataset,\n",
        "        model_display_name=MODEL_DISPLAY_NAME,\n",
        "        bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
        "        args=CMDARGS,\n",
        "        replica_count=1,\n",
        "        machine_type=TRAIN_COMPUTE,\n",
        "        accelerator_count=0,\n",
        "    )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:google.cloud.aiplatform.utils.source_utils:Training script copied to:\n",
            "gs://mibanell-iris-code/aiplatform-2021-09-19-18:57:31.513-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
            "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
            "gs://mibanell-iris-code/aiplatform-custom-training-2021-09-19-18:57:31.740 \n",
            "INFO:google.cloud.aiplatform.training_jobs:No dataset split provided. The service will use a default split.\n",
            "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
            "https://console.cloud.google.com/ai/platform/locations/us-central1/training/1685738242357329920?project=987533379656\n",
            "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/987533379656/locations/us-central1/trainingPipelines/1685738242357329920 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/987533379656/locations/us-central1/trainingPipelines/1685738242357329920 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/987533379656/locations/us-central1/trainingPipelines/1685738242357329920 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/987533379656/locations/us-central1/trainingPipelines/1685738242357329920 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/987533379656/locations/us-central1/trainingPipelines/1685738242357329920 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
            "https://console.cloud.google.com/ai/platform/locations/us-central1/training/6956286812520185856?project=987533379656\n",
            "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/987533379656/locations/us-central1/trainingPipelines/1685738242357329920 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob run completed. Resource name: projects/987533379656/locations/us-central1/trainingPipelines/1685738242357329920\n",
            "INFO:google.cloud.aiplatform.training_jobs:Model available at projects/987533379656/locations/us-central1/models/8054045818761510912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e7iIj_8bZD-"
      },
      "source": [
        "## Deploy model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycptpF4yD1tS",
        "outputId": "48b218b2-1f97-4ff2-bce2-55aaf5a881ac"
      },
      "source": [
        "DEPLOYED_NAME = \"iris_deployed-\" + TIMESTAMP\n",
        "\n",
        "TRAFFIC_SPLIT = {\"0\": 100}\n",
        "\n",
        "MIN_NODES = 1\n",
        "MAX_NODES = 1\n",
        "\n",
        "endpoint = model.deploy(\n",
        "        deployed_model_display_name=DEPLOYED_NAME,\n",
        "        traffic_split=TRAFFIC_SPLIT,\n",
        "        machine_type=DEPLOY_COMPUTE,\n",
        "        min_replica_count=MIN_NODES,\n",
        "        max_replica_count=MAX_NODES,\n",
        "    )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
            "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/987533379656/locations/us-central1/endpoints/7359708623867478016/operations/1037489001486876672\n",
            "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/987533379656/locations/us-central1/endpoints/7359708623867478016\n",
            "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
            "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/987533379656/locations/us-central1/endpoints/7359708623867478016')\n",
            "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/987533379656/locations/us-central1/endpoints/7359708623867478016\n",
            "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/987533379656/locations/us-central1/endpoints/7359708623867478016/operations/1141071792916398080\n",
            "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/987533379656/locations/us-central1/endpoints/7359708623867478016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6kmbEQ-a7Y5",
        "outputId": "0ef569d9-e93a-48db-bf0c-60359fc09040"
      },
      "source": [
        "# Prediction\n",
        "endpoint.predict(instances=[5.1, 2.5, 3.0, 1.1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(predictions=[[0.631752431, 0.123804167, 0.244443342]], deployed_model_id='8104869644243828736', explanations=None)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}